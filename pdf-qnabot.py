# -*- coding: utf-8 -*-
"""pdf-qnabot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fj1PtGdnnAip1fMVXpCiMv_BVrS6hKEm
"""

#!pip install streamlit langchain langchain_community openai PyPDF2 faiss-cpu tiktoken

import os
import streamlit as st
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from PyPDF2 import PdfReader
import tiktoken

# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="PDF ê¸°ë°˜ Q&A ì±—ë´‡", layout="wide")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("ì„¤ì •")
pdf = st.sidebar.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")
model_option = st.sidebar.selectbox(
    "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
    ("GPT-3.5-turbo", "GPT-4o-mini")
)

# ë©”ì¸ í™”ë©´ ì„¤ì •
st.title("PDF ê¸°ë°˜ Q&A ì±—ë´‡")

# OpenAI API í‚¤ ì„¤ì •
openai_api_key = st.secrets["openai_api_key"]
os.environ["OPENAI_API_KEY"] = openai_api_key

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "qa_history" not in st.session_state:
    st.session_state.qa_history = []
if "knowledge_base" not in st.session_state:
    st.session_state.knowledge_base = None

# PDF ì²˜ë¦¬ í•¨ìˆ˜
def process_pdf(pdf):
    pdf_reader = PdfReader(pdf)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()

    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)

    embeddings = OpenAIEmbeddings()
    st.session_state.knowledge_base = FAISS.from_texts(chunks, embeddings)
    st.sidebar.success("PDFê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")

# PDF ì—…ë¡œë“œ ì‹œ ì²˜ë¦¬
if pdf is not None and st.session_state.knowledge_base is None:
    process_pdf(pdf)
    
# ë©”ì‹œì§€ í‘œì‹œ í•¨ìˆ˜
def display_message(is_user, content):
    if is_user:
        icon = "ğŸ‘¤"
        icon_type = "user"
    else:
        icon = "ğŸ¤–"
        icon_type = "bot"

    message_html = f"""
    <div class="chat-message {icon_type}">
        <div class="avatar">
            {icon}
        </div>
        <div class="message">{content}</div>
    </div>
    """
    st.markdown(message_html, unsafe_allow_html=True)
    
# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
st.write("---")
if st.session_state.knowledge_base is not None:
    user_question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    if user_question:
        docs = st.session_state.knowledge_base.similarity_search(user_question)
        
        # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ LLM ì„¤ì •
        if model_option == "GPT-4o-mini":
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
        else:
            llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
        
        chain = load_qa_chain(llm, chain_type="stuff")
        response = chain.run(input_documents=docs, question=user_question)

        st.session_state.qa_history.append({"question": user_question, "answer": response})

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for qa in reversed(st.session_state.qa_history):
        message_container = st.container()
        with message_container:
            col1, col2 = st.columns([1, 9])
            with col1:
                st.image("https://via.placeholder.com/40x40.png?text=You", width=40)
            with col2:
                st.markdown(f"**You:** {qa['question']}")
            
            col1, col2 = st.columns([1, 9])
            with col1:
                st.image("https://via.placeholder.com/40x40.png?text=Bot", width=40)
            with col2:
                st.markdown(f"**Bot:** {qa['answer']}")
        st.write("---")
else:
    st.info("PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ í‘œì‹œ
st.sidebar.write(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {model_option}")
